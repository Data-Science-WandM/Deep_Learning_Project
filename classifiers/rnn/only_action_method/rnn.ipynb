{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('output_file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assume `data` is your DataFrame\n",
    "print(\"Original class distribution:\")\n",
    "print(data['user_class'].value_counts())\n",
    "\n",
    "# Step 1: Separate data by user_class\n",
    "bots = data[data['user_class'] == 'bot']\n",
    "humans = data[data['user_class'] == 'human']\n",
    "\n",
    "# Step 2: Balance the classes by downsampling the larger class\n",
    "min_class_size = min(len(bots), len(humans))\n",
    "\n",
    "# Downsample each class to the minimum class size\n",
    "bots_balanced = bots.sample(n=min_class_size, random_state=1)\n",
    "humans_balanced = humans.sample(n=min_class_size, random_state=1)\n",
    "\n",
    "# Step 3: Combine the balanced classes\n",
    "balanced_data = pd.concat([bots_balanced, humans_balanced])\n",
    "\n",
    "# Step 4: Shuffle the data\n",
    "balanced_data = shuffle(balanced_data, random_state=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced class distribution:\")\n",
    "print(balanced_data['user_class'].value_counts())\n",
    "\n",
    "data = balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data shape\", data.shape)\n",
    "print(\"columns\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, 'action_string']\n",
    "        label = 1 if self.data.loc[idx, 'user_class'] == 'bot' else 0\n",
    "        return {\n",
    "            'text': text, \n",
    "            'label': label \n",
    "        }\n",
    "\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Example usage: Iterate through the test loader\n",
    "# for batch in val_loader:\n",
    "#     print(batch['text'], batch['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter: subclass of Python's dictionary used for counting hashable objects, in this case, tokens (words).\n",
    "# OrderedDict: subclass of Python's dictionary that remembers the insertion order of keys. It is used to store tokens in a specific order based on frequency.\n",
    "from collections import Counter, OrderedDict\n",
    "# re: A module for working with regular expressions, used to manipulate and clean text.\n",
    "import re\n",
    "\n",
    "# Step 1: Token counts and vocab creation\n",
    "# Initializes an empty Counter object to hold the frequency of each token in the dataset.\n",
    "token_counts = Counter()\n",
    "\n",
    "# Define tokenizer\n",
    "def tokenizer(text):\n",
    "    # text = re.sub('<[^>]*>', '', text)  # Remove HTML tags\n",
    "\n",
    "    # This converts the entire text string to lowercase to ensure the regex matching is case-insensitive\n",
    "    # Emoticons are case-insensitive, so text.lower is not necessary\n",
    "    # emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())  # Extract emoticons\n",
    "\n",
    "    # \\W a shorthand in regex that matches any non-word character. Replace occurrences with space.\n",
    "    # text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '') #adds emoticons to the cleaned text.\n",
    "\n",
    "    #  creates a list of words (tokens)\n",
    "    # tokenized = text.replace(\" \", \"\").split(\"|\")\n",
    "\n",
    "    # return tokenized\n",
    "\n",
    "    return list(text)\n",
    "\n",
    "# Step 2: Tokenize the training data and populate token_counts\n",
    "for entry in test_dataset:  # Assuming train_dataset is a dataset with 'text'\n",
    "    line = entry['text']\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Step 3: Sort tokens by frequency\n",
    "# token_counts.items() returns the tokens and their respective counts as a list of tuples (e.g., [(token1, count1), (token2, count2), ...])\n",
    "# key=lambda x: x[1] means that the sorting is based on the count (x[1]), which is the second element of each tuple\n",
    "# reverse=True means that the most frequent tokens appear first in the sorted list.\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Limit the vocabulary to the top 69023 tokens (including special tokens)\n",
    "# The padding token (pad) is used to ensure that all sequences in a batch have the same length.\n",
    "# The unknown token (unk) is used to represent words that are not found in the model's vocabulary (the top 69021 words in your case).\n",
    "# Any word that doesn't appear in the vocabulary is replaced by the unk token during tokenization.\n",
    "# This is critical for handling unseen words during inference, where the model encounters words that were not present in the training data.\n",
    "limited_sorted_by_freq_tuples = sorted_by_freq_tuples[:69021]  # Top 69021 + pad and unk\n",
    "\n",
    "# Step 5: Create an ordered dictionary for the vocab\n",
    "ordered_dict = OrderedDict(limited_sorted_by_freq_tuples)\n",
    "\n",
    "# Step 6: Create vocab dictionary with special tokens\n",
    "# Initializes the vocab dictionary with two special tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "for idx, (token, count) in enumerate(ordered_dict.items(), start=2):  # Start from 2 to skip the special tokens\n",
    "    vocab[token] = idx\n",
    "\n",
    "\n",
    "# Print the vocabulary size (should be 69023)\n",
    "print('Vocab-size:', len(vocab))\n",
    "print('vocab', vocab)\n",
    "# --- Rationale:\n",
    "#\n",
    "# By assigning frequent words lower indices, we can optimize memory and computational efficiency.\n",
    "# Words that appear infrequently can either be assigned higher indices (in case we want to keep them) or omitted from the vocabulary entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    #If the token does not exist in the vocab, the function returns the index of the <unk>\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "# Example usage\n",
    "print(encode(list('rr|⚁rr|r⚁r|⚁rr|r□r|□r⚀T|⚀T⚀T|⚁rr|r□r|⚀rr|⚁T⚀T|⚀T⚀T|□rr|r□T|r⚀r|⚀r□r|⚀r⚀r|⚀p⚁r|□p⚀p|⚀T⚀r|□rr|⚁rr|□r□r|□rr|Tr|rr|r⚁r|⚀T⚁r|⚁T⚁r|r⚀r|r⚁r|⚀rr|r⚁r|⚀r⚁r|rr|⚁T⚀π|⚁T□r|r⚀T|⚀pr|⚁rr|r⚀T|⚀T□r|□p⚁r|□p⚀T|⚀rr|rr|□r⚁T|⚀T⚁r|rr|⚀T⚀r|⚀π⚁T|□r⚁r|⚁r□r|rr|⚀πp|⚁r⚁r|□r⚀T|⚀rr|r⚀r|rr|r⚁π|⚀r□p|□r⚁r|□r⚀p|□r⚀p|⚁p⚁r|rr|r□r|□r⚀r|r⚁r|⚀T⚀T|⚀r⚀r|⚁rr|rr|r⚀r|□r⚁r|rr|rr|⚀r⚁r|⚁r□r|⚁r□p|⚀r⚀r|□rr|r⚀π|⚀T⚀T|⚁rr|rr|rr|rr|rr|⚀T⚁r|⚀rp|rp|□r⚀r|□r⚀p|⚀r⚀r|⚀r□r|□r□r|rr|□r□r|⚀r⚁r|⚀rr|r□r|□rr|r□r|r□r|⚀r□r|⚀p⚀r|□r□r|⚀r⚀p|□rr|r⚀r|⚀r⚀r|□r□r|r⚁r|⚀r⚀r|⚀r□r|⚀r□r|⚀r⚁r|⚀r⚀r|⚀p⚀r|□r⚀r|⚀r□r|⚀T□T|□rr|□rr|r□r|□rr|r□T|pr|□rr|rr|⚀r⚀p|⚀r⚀r|⚀rr|□r□r|□rr|□rr|r□r|□r⚀r|⚀pr|□rr|rr|rr|rr|rr|rp|⚀r⚀r|r⚀r|□r⚀r')))  # Should output something like [11, 7, 35, 457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: this code may be very slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the manual vocab creation process from earlier\n",
    "# Assuming `vocab` and `tokenizer` are already defined\n",
    "\n",
    "#text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "# Updated text pipeline\n",
    "text_pipeline = lambda x: [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(x)]\n",
    "\n",
    "label_pipeline = lambda x: float(x)  # Convert to float to match the output\n",
    "\n",
    "# Batch collation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for entry in batch:  # Each 'entry' is a dictionary with 'text' and 'label'\n",
    "        _label = entry['label']\n",
    "        _text = entry['text']\n",
    "\n",
    "        # Process labels and text\n",
    "        label_list.append(label_pipeline(_label))  # Convert labels using label_pipeline\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # Convert text to indices\n",
    "\n",
    "        # Store processed text and its length\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # Convert lists to tensors and pad sequences\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----  Example usage with DataLoader -----#\n",
    "## Take a small batch\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "\n",
    "# Print the output batch\n",
    "print(\"Text batch:\", text_batch)\n",
    "print(\"Label batch:\", label_batch)\n",
    "print(\"Length batch:\", length_batch)\n",
    "print(\"Text batch shape:\", text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batching the datasets\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, the embedding layer creates the number of features that will be fed into the RNN\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,\n",
    "                                      embed_dim,\n",
    "                                      padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
    "                           batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store training and validation metrics for each epoch\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "valid_accuracies = []\n",
    "valid_losses = []\n",
    "\n",
    "num_epochs = 10\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Training loop with metrics storage\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(val_dl)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_accuracies.append(acc_train)\n",
    "    train_losses.append(loss_train)\n",
    "    valid_accuracies.append(acc_valid)\n",
    "    valid_losses.append(loss_valid)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} - accuracy: {acc_train:.4f}, val_accuracy: {acc_valid:.4f}')\n",
    "\n",
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, _ = evaluate(test_dl)\n",
    "print(f'test_accuracy: {acc_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Function to preprocess and predict a single comment\n",
    "def predict_comment(text, model, vocab):\n",
    "    model.eval()\n",
    "    # Tokenize and encode the input text using the same tokenizer and vocab as used during training\n",
    "    tokens = tokenizer(text)\n",
    "    encoded_text = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "    # Convert the tokens to tensor and add batch dimension\n",
    "    text_tensor = torch.tensor(encoded_text).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    lengths_tensor = torch.tensor([len(encoded_text)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(text_tensor, lengths_tensor)[:, 0]\n",
    "\n",
    "    # Apply threshold of 0.5 for binary classification\n",
    "    prediction_label = 1 if prediction >= 0.5 else 0\n",
    "    return prediction.item(), prediction_label\n",
    "\n",
    "# List to store prediction results\n",
    "results = []\n",
    "\n",
    "# Iterate over all samples in the test dataset\n",
    "for sample in test_dataset:\n",
    "    comment_text = sample['text']\n",
    "    true_label = sample['label']\n",
    "    predicted_value, predicted_label = predict_comment(comment_text, model, vocab)\n",
    "\n",
    "    # Append the data for each comment\n",
    "    results.append({\n",
    "        \"comment\": comment_text,\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_value\": predicted_value,\n",
    "        \"predicted_label\": predicted_label\n",
    "    })\n",
    "\n",
    "# Convert results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix on the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Collect predictions and true labels from the validation set\n",
    "for sample in test_dataset:\n",
    "    comment_text = sample['text']\n",
    "    true_label = sample['label']\n",
    "    predicted_value, predicted_label = predict_comment(comment_text, model, vocab)  # Adjust if predict_comment takes a batch\n",
    "    all_preds.append(predicted_label)\n",
    "    all_labels.append(true_label)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['true_label'] != results_df['predicted_label']].to_csv(\"mismatches.csv\")\n",
    "results_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
