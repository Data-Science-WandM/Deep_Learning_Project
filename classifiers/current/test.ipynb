{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Define a basic RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        rnn_out, _ = self.rnn(packed)\n",
    "        padded_out, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        out = self.fc(self.dropout(padded_out[:, -1, :]))  # Use the last time-step\n",
    "        return out\n",
    "\n",
    "# Dataset class for text classification\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_len, vocab, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokenized_text, length = self.tokenize_and_pad(text)\n",
    "        return tokenized_text, label, length\n",
    "\n",
    "    def tokenize_and_pad(self, text):\n",
    "        # Tokenization: convert text to tokens using vocab\n",
    "        tokens = self.tokenizer(text)\n",
    "        length = len(tokens)\n",
    "        # Padding (or truncating) the sequence to max_len\n",
    "        if length < self.max_len:\n",
    "            tokens = tokens + [0] * (self.max_len - length)  # Pad with 0\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]  # Truncate if too long\n",
    "        return tokens, length\n",
    "\n",
    "# Training function for each epoch\n",
    "def train_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n",
    "        lengths = torch.tensor(lengths).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)\n",
    "        \n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = accuracy_score(label_batch.cpu().numpy(), pred.argmax(dim=1).cpu().numpy())\n",
    "        total_acc += acc\n",
    "\n",
    "    return total_acc / len(dataloader), total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function for each epoch\n",
    "def evaluate_epoch(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch, label_batch = text_batch.to(device), label_batch.to(device)\n",
    "            lengths = torch.tensor(lengths).to(device)\n",
    "\n",
    "            pred = model(text_batch, lengths)\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            acc = accuracy_score(label_batch.cpu().numpy(), pred.argmax(dim=1).cpu().numpy())\n",
    "            total_acc += acc\n",
    "\n",
    "    return total_acc / len(dataloader), total_loss / len(dataloader)\n",
    "\n",
    "# Example tokenizer (replace with your actual tokenizer)\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(texts, labels, vocab, tokenizer, max_len=128, batch_size=32, num_epochs=10, hidden_dim=128, learning_rate=0.001):\n",
    "    # Create dataset and dataloaders\n",
    "    train_dataset = TextDataset(texts, labels, max_len, vocab, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = RNNModel(vocab_size=len(vocab), embedding_dim=100, hidden_dim=hidden_dim, output_dim=len(set(labels)))\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        acc_train, loss_train = train_epoch(train_loader, model, optimizer, loss_fn)\n",
    "        train_accuracies.append(acc_train)\n",
    "        train_losses.append(loss_train)\n",
    "\n",
    "        # Evaluate on validation set (can use another DataLoader for validation)\n",
    "        acc_valid, loss_valid = evaluate_epoch(train_loader, model, loss_fn)\n",
    "        val_accuracies.append(acc_valid)\n",
    "        val_losses.append(loss_valid)\n",
    "\n",
    "        print(f\"Train Accuracy: {acc_train:.4f}, Train Loss: {loss_train:.4f}\")\n",
    "        print(f\"Validation Accuracy: {acc_valid:.4f}, Validation Loss: {loss_valid:.4f}\")\n",
    "    \n",
    "    return model, train_accuracies, val_accuracies, train_losses, val_losses\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "texts = [\"this is a sentence\", \"another example sentence\", \"deep learning is fun\"]\n",
    "labels = [0, 1, 0]\n",
    "vocab = {'<PAD>': 0, 'this': 1, 'is': 2, 'a': 3, 'sentence': 4, 'another': 5, 'example': 6, 'deep': 7, 'learning': 8, 'fun': 9}\n",
    "tokenizer = simple_tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "model, train_accuracies, val_accuracies, train_losses, val_losses = train_and_evaluate(texts, labels, vocab, tokenizer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
