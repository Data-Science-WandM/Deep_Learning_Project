{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ddc186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:03.022238Z",
     "iopub.status.busy": "2024-12-14T20:12:03.021829Z",
     "iopub.status.idle": "2024-12-14T20:12:05.049953Z",
     "shell.execute_reply": "2024-12-14T20:12:05.049252Z"
    },
    "papermill": {
     "duration": 2.033706,
     "end_time": "2024-12-14T20:12:05.050897",
     "exception": false,
     "start_time": "2024-12-14T20:12:03.017191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /sciclone/data10/iahewababarand/.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57961ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:05.059398Z",
     "iopub.status.busy": "2024-12-14T20:12:05.059136Z",
     "iopub.status.idle": "2024-12-14T20:12:06.904018Z",
     "shell.execute_reply": "2024-12-14T20:12:06.903399Z"
    },
    "papermill": {
     "duration": 1.850148,
     "end_time": "2024-12-14T20:12:06.905203",
     "exception": false,
     "start_time": "2024-12-14T20:12:05.055055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import re\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a36291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:06.913396Z",
     "iopub.status.busy": "2024-12-14T20:12:06.913078Z",
     "iopub.status.idle": "2024-12-14T20:12:13.960772Z",
     "shell.execute_reply": "2024-12-14T20:12:13.960261Z"
    },
    "papermill": {
     "duration": 7.052572,
     "end_time": "2024-12-14T20:12:13.961709",
     "exception": false,
     "start_time": "2024-12-14T20:12:06.909137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_syntactic_blocstring</th>\n",
       "      <th>action_blocstring</th>\n",
       "      <th>changes_list_content_syntactic_</th>\n",
       "      <th>changes_list_action</th>\n",
       "      <th>src</th>\n",
       "      <th>userId</th>\n",
       "      <th>user_class</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>change_content_syntactic</th>\n",
       "      <th>change_action</th>\n",
       "      <th>change_change_dynamic_score</th>\n",
       "      <th>highest_change_in_content_syntactic</th>\n",
       "      <th>lowest_change_in_content_syntactic</th>\n",
       "      <th>standard_deviation_of_content_syntactic</th>\n",
       "      <th>highest_change_in_action</th>\n",
       "      <th>lowest_change_in_action</th>\n",
       "      <th>standard_deviation_of_action</th>\n",
       "      <th>diversity_action</th>\n",
       "      <th>diversity_content_syntactic</th>\n",
       "      <th>diversity_change_dynamics_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mmmmmmmmqt)(mmmmmmmmqt)(mmmmmmmmqt)(mmmmmmmmq...</td>\n",
       "      <td>rprp⚀pr□prrprrrprpprprprprprpprprprp□rprprprrr...</td>\n",
       "      <td>[0.683772233983162, 0.00647286670992131, 0.801...</td>\n",
       "      <td>{'content_syntactic': [0.683772233983162, 0.00...</td>\n",
       "      <td>astroturf</td>\n",
       "      <td>146048090</td>\n",
       "      <td>bot</td>\n",
       "      <td>274</td>\n",
       "      <td>0.556376</td>\n",
       "      <td>0.297313</td>\n",
       "      <td>1.024345</td>\n",
       "      <td>0.938307</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.275047</td>\n",
       "      <td>0.901294</td>\n",
       "      <td>0.054951</td>\n",
       "      <td>0.200933</td>\n",
       "      <td>0.653102</td>\n",
       "      <td>0.652532</td>\n",
       "      <td>0.450906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(mmmmmmmqt)(mmmmmmmqt)(mmmmmmmqt)(mmmmmmmqt)(m...</td>\n",
       "      <td>r□pr□rr□rp⚀r⚀TTTTTTT□r⚀p⚀π□p|⚀rr⚀rr⚀rr□r⚀r|⚀p⚀...</td>\n",
       "      <td>[0.0600874600144512, 0.022936062507937005, 0.0...</td>\n",
       "      <td>{'content_syntactic': [0.0600874600144512, 0.0...</td>\n",
       "      <td>astroturf</td>\n",
       "      <td>797927149856403456</td>\n",
       "      <td>bot</td>\n",
       "      <td>275</td>\n",
       "      <td>0.427565</td>\n",
       "      <td>0.484909</td>\n",
       "      <td>0.711994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004220</td>\n",
       "      <td>0.356787</td>\n",
       "      <td>0.989180</td>\n",
       "      <td>0.068479</td>\n",
       "      <td>0.291316</td>\n",
       "      <td>0.707387</td>\n",
       "      <td>0.693840</td>\n",
       "      <td>0.151417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(mmt)(mmt)(qt)(qt)(qt)(mqt)(qt)(t)(qt)(qt)(Em)...</td>\n",
       "      <td>r□r⚀r⚀rr□rrr□r□r⚀rrr⚀r|⚁rrrrr□rrrrrr□rpprrrp□r...</td>\n",
       "      <td>[0.4050577935998917, 0.2173762078750736, 0.181...</td>\n",
       "      <td>{'content_syntactic': [0.4050577935998917, 0.2...</td>\n",
       "      <td>astroturf</td>\n",
       "      <td>1046169889138868225</td>\n",
       "      <td>bot</td>\n",
       "      <td>277</td>\n",
       "      <td>0.442671</td>\n",
       "      <td>0.284123</td>\n",
       "      <td>1.405920</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.265496</td>\n",
       "      <td>0.913974</td>\n",
       "      <td>0.045573</td>\n",
       "      <td>0.204972</td>\n",
       "      <td>0.676182</td>\n",
       "      <td>0.644029</td>\n",
       "      <td>0.297367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mmt)(mmmmmmqt)(mmmmmmmmmqt)(mmt)(mmt)(qt)(mmm...</td>\n",
       "      <td>prrpp□rrrrrrrrrrrrrr□rr□rrr□rrprrrrrrr□prrrrrr...</td>\n",
       "      <td>[0.6288092648051271, 0.17944110183186945, 1.0,...</td>\n",
       "      <td>{'content_syntactic': [0.6288092648051271, 0.1...</td>\n",
       "      <td>astroturf</td>\n",
       "      <td>1085010463128195073</td>\n",
       "      <td>bot</td>\n",
       "      <td>244</td>\n",
       "      <td>0.368786</td>\n",
       "      <td>0.419595</td>\n",
       "      <td>1.029069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111362</td>\n",
       "      <td>0.206134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049906</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.698081</td>\n",
       "      <td>0.620676</td>\n",
       "      <td>0.211830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(mmmmmmmqt)|(Em)|(t)(mmqt)(mt)(mmt)|(qt)(HUqt)...</td>\n",
       "      <td>p|⚁p|⚀p□p□p□p|⚁rrrrrrrrpr□prrprr□rrrprrprrrprp...</td>\n",
       "      <td>[1.0, 1.0, 0.7298648986655512, 0.8616571072267...</td>\n",
       "      <td>{'content_syntactic': [1.0, 1.0, 0.72986489866...</td>\n",
       "      <td>astroturf</td>\n",
       "      <td>1613166488</td>\n",
       "      <td>bot</td>\n",
       "      <td>245</td>\n",
       "      <td>0.618332</td>\n",
       "      <td>0.353069</td>\n",
       "      <td>0.605766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209196</td>\n",
       "      <td>0.253005</td>\n",
       "      <td>0.849471</td>\n",
       "      <td>0.075654</td>\n",
       "      <td>0.199506</td>\n",
       "      <td>0.680286</td>\n",
       "      <td>0.766445</td>\n",
       "      <td>0.627025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        content_syntactic_blocstring  \\\n",
       "0  (mmmmmmmmqt)(mmmmmmmmqt)(mmmmmmmmqt)(mmmmmmmmq...   \n",
       "1  (mmmmmmmqt)(mmmmmmmqt)(mmmmmmmqt)(mmmmmmmqt)(m...   \n",
       "2  (mmt)(mmt)(qt)(qt)(qt)(mqt)(qt)(t)(qt)(qt)(Em)...   \n",
       "3  (mmt)(mmmmmmqt)(mmmmmmmmmqt)(mmt)(mmt)(qt)(mmm...   \n",
       "4  (mmmmmmmqt)|(Em)|(t)(mmqt)(mt)(mmt)|(qt)(HUqt)...   \n",
       "\n",
       "                                   action_blocstring  \\\n",
       "0  rprp⚀pr□prrprrrprpprprprprprpprprprp□rprprprrr...   \n",
       "1  r□pr□rr□rp⚀r⚀TTTTTTT□r⚀p⚀π□p|⚀rr⚀rr⚀rr□r⚀r|⚀p⚀...   \n",
       "2  r□r⚀r⚀rr□rrr□r□r⚀rrr⚀r|⚁rrrrr□rrrrrr□rpprrrp□r...   \n",
       "3  prrpp□rrrrrrrrrrrrrr□rr□rrr□rrprrrrrrr□prrrrrr...   \n",
       "4  p|⚁p|⚀p□p□p□p|⚁rrrrrrrrpr□prrprr□rrrprrprrrprp...   \n",
       "\n",
       "                     changes_list_content_syntactic_  \\\n",
       "0  [0.683772233983162, 0.00647286670992131, 0.801...   \n",
       "1  [0.0600874600144512, 0.022936062507937005, 0.0...   \n",
       "2  [0.4050577935998917, 0.2173762078750736, 0.181...   \n",
       "3  [0.6288092648051271, 0.17944110183186945, 1.0,...   \n",
       "4  [1.0, 1.0, 0.7298648986655512, 0.8616571072267...   \n",
       "\n",
       "                                 changes_list_action        src  \\\n",
       "0  {'content_syntactic': [0.683772233983162, 0.00...  astroturf   \n",
       "1  {'content_syntactic': [0.0600874600144512, 0.0...  astroturf   \n",
       "2  {'content_syntactic': [0.4050577935998917, 0.2...  astroturf   \n",
       "3  {'content_syntactic': [0.6288092648051271, 0.1...  astroturf   \n",
       "4  {'content_syntactic': [1.0, 1.0, 0.72986489866...  astroturf   \n",
       "\n",
       "                userId user_class  tweet_count  change_content_syntactic  \\\n",
       "0            146048090        bot          274                  0.556376   \n",
       "1   797927149856403456        bot          275                  0.427565   \n",
       "2  1046169889138868225        bot          277                  0.442671   \n",
       "3  1085010463128195073        bot          244                  0.368786   \n",
       "4           1613166488        bot          245                  0.618332   \n",
       "\n",
       "   change_action  change_change_dynamic_score  \\\n",
       "0       0.297313                     1.024345   \n",
       "1       0.484909                     0.711994   \n",
       "2       0.284123                     1.405920   \n",
       "3       0.419595                     1.029069   \n",
       "4       0.353069                     0.605766   \n",
       "\n",
       "   highest_change_in_content_syntactic  lowest_change_in_content_syntactic  \\\n",
       "0                             0.938307                            0.006473   \n",
       "1                             1.000000                            0.004220   \n",
       "2                             1.000000                            0.015253   \n",
       "3                             1.000000                            0.111362   \n",
       "4                             1.000000                            0.209196   \n",
       "\n",
       "   standard_deviation_of_content_syntactic  highest_change_in_action  \\\n",
       "0                                 0.275047                  0.901294   \n",
       "1                                 0.356787                  0.989180   \n",
       "2                                 0.265496                  0.913974   \n",
       "3                                 0.206134                  1.000000   \n",
       "4                                 0.253005                  0.849471   \n",
       "\n",
       "   lowest_change_in_action  standard_deviation_of_action  diversity_action  \\\n",
       "0                 0.054951                      0.200933          0.653102   \n",
       "1                 0.068479                      0.291316          0.707387   \n",
       "2                 0.045573                      0.204972          0.676182   \n",
       "3                 0.049906                      0.306818          0.698081   \n",
       "4                 0.075654                      0.199506          0.680286   \n",
       "\n",
       "   diversity_content_syntactic  diversity_change_dynamics_score  \n",
       "0                     0.652532                         0.450906  \n",
       "1                     0.693840                         0.151417  \n",
       "2                     0.644029                         0.297367  \n",
       "3                     0.620676                         0.211830  \n",
       "4                     0.766445                         0.627025  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(filename):\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "data = get_data('output_file copy.json')\n",
    "# data = get_data('./rnn/only_action_method/output_file copy.json')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79634415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:13.971058Z",
     "iopub.status.busy": "2024-12-14T20:12:13.970426Z",
     "iopub.status.idle": "2024-12-14T20:12:14.051490Z",
     "shell.execute_reply": "2024-12-14T20:12:14.050901Z"
    },
    "papermill": {
     "duration": 0.086332,
     "end_time": "2024-12-14T20:12:14.052311",
     "exception": false,
     "start_time": "2024-12-14T20:12:13.965979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "user_class\n",
      "bot      32041\n",
      "human    27704\n",
      "Name: count, dtype: int64\n",
      "Balanced class distribution:\n",
      "user_class\n",
      "human    27704\n",
      "bot      27704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original class distribution:\")\n",
    "print(data['user_class'].value_counts())\n",
    "\n",
    "# Separate data by user_class\n",
    "bots = data[data['user_class'] == 'bot']\n",
    "humans = data[data['user_class'] == 'human']\n",
    "\n",
    "# Select the minimum class size\n",
    "min_class_size = min(len(bots), len(humans))\n",
    "\n",
    "# Downsample each class to the minimum class size\n",
    "bots_balanced = bots.sample(n=min_class_size, random_state=1)\n",
    "humans_balanced = humans.sample(n=min_class_size, random_state=1)\n",
    "\n",
    "# Combine the balanced classes\n",
    "balanced_data = pd.concat([bots_balanced, humans_balanced])\n",
    "\n",
    "# Shuffle the data\n",
    "balanced_data = shuffle(balanced_data, random_state=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced class distribution:\")\n",
    "print(balanced_data['user_class'].value_counts())\n",
    "\n",
    "data = balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3cd74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['astroturf', 'botometer-feedback-19', 'botwiki-19', 'celebrity-19',\n",
       "       'cresci-17', 'cresci-rtbust-19', 'cresci-stock-18', 'gilani-17',\n",
       "       'midterm-18', 'political-bots-19', 'pronbots-19', 'varol-17',\n",
       "       'vendor-purchased-19', 'verified-19'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(data['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e57e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:14.061194Z",
     "iopub.status.busy": "2024-12-14T20:12:14.060498Z",
     "iopub.status.idle": "2024-12-14T20:12:14.063631Z",
     "shell.execute_reply": "2024-12-14T20:12:14.063261Z"
    },
    "papermill": {
     "duration": 0.00833,
     "end_time": "2024-12-14T20:12:14.064355",
     "exception": false,
     "start_time": "2024-12-14T20:12:14.056025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"data shape\", data.shape)\n",
    "print(\"columns\", data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3427022c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:14.073125Z",
     "iopub.status.busy": "2024-12-14T20:12:14.072635Z",
     "iopub.status.idle": "2024-12-14T20:12:14.102366Z",
     "shell.execute_reply": "2024-12-14T20:12:14.102023Z"
    },
    "papermill": {
     "duration": 0.035112,
     "end_time": "2024-12-14T20:12:14.103098",
     "exception": false,
     "start_time": "2024-12-14T20:12:14.067986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UserDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, 'action_blocstring'][:200]\n",
    "        label = 1 if self.data.loc[idx, 'user_class'] == 'bot' else 0\n",
    "        return {\n",
    "            'text': text, \n",
    "            'label': label \n",
    "        }\n",
    "\n",
    "dataset = UserDataset(data)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Example usage: Iterate through the test loader\n",
    "# for batch in val_loader:\n",
    "#     print(batch['text'], batch['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70600ef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:14.112188Z",
     "iopub.status.busy": "2024-12-14T20:12:14.111517Z",
     "iopub.status.idle": "2024-12-14T20:12:15.300367Z",
     "shell.execute_reply": "2024-12-14T20:12:15.299563Z"
    },
    "papermill": {
     "duration": 1.194239,
     "end_time": "2024-12-14T20:12:15.301317",
     "exception": false,
     "start_time": "2024-12-14T20:12:14.107078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counter: subclass of Python's dictionary used for counting hashable objects, in this case, tokens (words).\n",
    "# OrderedDict: subclass of Python's dictionary that remembers the insertion order of keys. It is used to store tokens in a specific order based on frequency.\n",
    "from collections import Counter, OrderedDict\n",
    "# re: A module for working with regular expressions, used to manipulate and clean text.\n",
    "import re\n",
    "\n",
    "# Token counts and vocab creation\n",
    "# Initializes an empty Counter object to hold the frequency of each token in the dataset.\n",
    "token_counts = Counter()\n",
    "\n",
    "# Define tokenizer\n",
    "def tokenizer(text):\n",
    "\n",
    "    #  replace | with \" \"\n",
    "    # text = text.replace(\"|\", \" \")\n",
    "\n",
    "    return list(text)\n",
    "\n",
    "# Tokenize the training data and populate token_counts\n",
    "for entry in train_dataset:  # Assuming train_dataset is a dataset with 'text'\n",
    "    line = entry['text']\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Sort tokens by frequency\n",
    "# token_counts.items() returns the tokens and their respective counts as a list of tuples (e.g., [(token1, count1), (token2, count2), ...])\n",
    "# key=lambda x: x[1] means that the sorting is based on the count (x[1]), which is the second element of each tuple\n",
    "# reverse=True means that the most frequent tokens appear first in the sorted list.\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create an ordered dictionary for the vocab\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# The padding token (pad) is used to ensure that all sequences in a batch have the same length.\n",
    "# The unknown token (unk) is used to represent words that are not found in the model's vocabulary (the top 69021 words in your case).\n",
    "# Any word that doesn't appear in the vocabulary is replaced by the unk token during tokenization.\n",
    "# This is critical for handling unseen words during inference, where the model encounters words that were not present in the training data.\n",
    "# Create vocab dictionary with special tokens\n",
    "# Initializes the vocab dictionary with two special tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "for idx, (token, count) in enumerate(ordered_dict.items(), start=2):  # Start from 2 to skip the special tokens\n",
    "    vocab[token] = idx\n",
    "\n",
    "\n",
    "# Print the vocabulary size (should be 69023)\n",
    "print('Vocab-size:', len(vocab))\n",
    "print('vocab', vocab)\n",
    "# --- Rationale:\n",
    "#\n",
    "# By assigning frequent words lower indices, we can optimize memory and computational efficiency.\n",
    "# Words that appear infrequently can either be assigned higher indices (in case we want to keep them) or omitted from the vocabulary entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd9e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:15.310511Z",
     "iopub.status.busy": "2024-12-14T20:12:15.310100Z",
     "iopub.status.idle": "2024-12-14T20:12:15.314313Z",
     "shell.execute_reply": "2024-12-14T20:12:15.313734Z"
    },
    "papermill": {
     "duration": 0.009657,
     "end_time": "2024-12-14T20:12:15.315188",
     "exception": false,
     "start_time": "2024-12-14T20:12:15.305531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# action T|⚂T|⚅T□TT□r⚀r⚀r|⚀r|⚀r□r⚀r|⚂rTT□r□r⚀r□r|⚀r⚀r⚀r\n",
    "\n",
    "# content (t)|(t)|(Et)(E)(Et)(qt)(Et)(EHUt)|(Et)|(mUt)(HHHHHHt)(qt)|(qt)(E)(Et)(mmmqt)(Et)(HUt)(Ut)|(qt)(mqt)(EHUt)\n",
    "\n",
    "# text (T -> t)|(⚂)(T -> t)|(⚅)(T -> Et)(□)(T -> E)(T -> Et)(□)(r -> qt)(⚀)(r -> Et)(⚀)(r -> EHUt)|(⚀)(r -> Et)|(⚀)(r -> mUt)(□)\n",
    "\n",
    "def encode(tokens):\n",
    "    #If the token does not exist in the vocab, the function returns the index of the <unk>\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "# Example usage\n",
    "print(encode(tokenizer('(T -> t)|(⚂)(T -> t)|(⚅)(T -> Et)(□)(T -> E)(T -> Et)(□)(r -> qt)(⚀)(r -> Et)(⚀)(r -> EHUt)|(⚀)(r -> Et)|(⚀)(r -> mUt)(□)')))  # Should output something like [11, 7, 35, 457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d60e77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:15.324181Z",
     "iopub.status.busy": "2024-12-14T20:12:15.323726Z",
     "iopub.status.idle": "2024-12-14T20:12:15.374707Z",
     "shell.execute_reply": "2024-12-14T20:12:15.374134Z"
    },
    "papermill": {
     "duration": 0.056302,
     "end_time": "2024-12-14T20:12:15.375482",
     "exception": false,
     "start_time": "2024-12-14T20:12:15.319180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: this code may be very slow on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a2afca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:15.385019Z",
     "iopub.status.busy": "2024-12-14T20:12:15.384498Z",
     "iopub.status.idle": "2024-12-14T20:12:15.389387Z",
     "shell.execute_reply": "2024-12-14T20:12:15.389032Z"
    },
    "papermill": {
     "duration": 0.010479,
     "end_time": "2024-12-14T20:12:15.390169",
     "exception": false,
     "start_time": "2024-12-14T20:12:15.379690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use the manual vocab creation process from earlier\n",
    "# Assuming `vocab` and `tokenizer` are already defined\n",
    "\n",
    "#text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "# Updated text pipeline\n",
    "text_pipeline = lambda x: [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(x)]\n",
    "\n",
    "label_pipeline = lambda x: float(x)  # Convert to float to match the output\n",
    "\n",
    "# Batch collation function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for entry in batch:  # Each 'entry' is a dictionary with 'text' and 'label'\n",
    "        _label = entry['label']\n",
    "        _text = entry['text']\n",
    "\n",
    "        # Process labels and text\n",
    "        label_list.append(label_pipeline(_label))  # Convert labels using label_pipeline\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # Convert text to indices\n",
    "\n",
    "        # Store processed text and its length\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "\n",
    "    # Convert lists to tensors and pad sequences\n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b4a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:15.398967Z",
     "iopub.status.busy": "2024-12-14T20:12:15.398597Z",
     "iopub.status.idle": "2024-12-14T20:12:15.761907Z",
     "shell.execute_reply": "2024-12-14T20:12:15.761286Z"
    },
    "papermill": {
     "duration": 0.368586,
     "end_time": "2024-12-14T20:12:15.762718",
     "exception": false,
     "start_time": "2024-12-14T20:12:15.394132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#-----  Example usage with DataLoader -----#\n",
    "## Take a small batch\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
    "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
    "\n",
    "# Print the output batch\n",
    "print(\"Text batch:\", text_batch)\n",
    "print(\"Label batch:\", label_batch)\n",
    "print(\"Length batch:\", length_batch)\n",
    "print(\"Text batch shape:\", text_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02cf9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batching the datasets\n",
    "batch_size = 32\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                      shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                     shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c68bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, num_layers=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, num_layers=num_layers, \n",
    "                           batch_first=True, dropout=dropout_rate)  # Add num_layers and dropout between layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # Explicit dropout layer\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # Embedding Layer\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        # Pack the padded sequence\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), \n",
    "                                                enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        # LSTM Layers\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        \n",
    "        # Take the hidden state of the last layer\n",
    "        out = hidden[-1, :, :]  # Shape: (batch_size, rnn_hidden_size)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Dropout for FC layer\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d3abc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_epoch(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:  # Loop through batches in dataloader\n",
    "            pred = model(text_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c68902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T20:12:15.772644Z",
     "iopub.status.busy": "2024-12-14T20:12:15.772380Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-12-14T20:12:15.767155",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning\n",
    "import itertools\n",
    "\n",
    "# Hyperparameter ranges\n",
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'embed_dim': [16, 32, 64],\n",
    "    'rnn_hidden_size': [32, 64, 128],\n",
    "    'fc_hidden_size': [32, 64, 128],\n",
    "    'num_layers': [1, 2, 3],  # Add num_layers\n",
    "    'dropout_rate': [0, 0.3, 0.5]  # Add dropout_rate\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(\n",
    "    param_grid['lr'],\n",
    "    param_grid['embed_dim'],\n",
    "    param_grid['rnn_hidden_size'],\n",
    "    param_grid['fc_hidden_size'],\n",
    "    param_grid['num_layers'],\n",
    "    param_grid['dropout_rate']\n",
    "))\n",
    "\n",
    "# Track results\n",
    "results = []\n",
    "\n",
    "# Grid search loop\n",
    "for lr, embed_dim, rnn_hidden_size, fc_hidden_size, num_layers, dropout_rate in param_combinations:\n",
    "    print(f\"Testing with lr={lr}, embed_dim={embed_dim}, rnn_hidden_size={rnn_hidden_size}, \"\n",
    "          f\"fc_hidden_size={fc_hidden_size}, num_layers={num_layers}, dropout_rate={dropout_rate}\")\n",
    "    \n",
    "    # Initialize model and optimizer for current hyperparameters\n",
    "    model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, \n",
    "                num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    # Train and evaluate for a few epochs (to save time during grid search)\n",
    "    for epoch in range(5):  # Shortened training for grid search\n",
    "        train_epoch(train_dl, model, optimizer, loss_fn)\n",
    "\n",
    "    acc_valid, _ = evaluate_epoch(val_dl, model, loss_fn)\n",
    "\n",
    "    print({\n",
    "        'lr': lr,\n",
    "        'embed_dim': embed_dim,\n",
    "        'rnn_hidden_size': rnn_hidden_size,\n",
    "        'fc_hidden_size': fc_hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'val_accuracy': acc_valid\n",
    "    })\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'embed_dim': embed_dim,\n",
    "        'rnn_hidden_size': rnn_hidden_size,\n",
    "        'fc_hidden_size': fc_hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'val_accuracy': acc_valid\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best hyperparameter combination\n",
    "best_params = results_df.loc[results_df['val_accuracy'].idxmax()]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Save the results for future reference\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8083ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr                   = 0.001\n",
    "# embed_dim            = 64\n",
    "# rnn_hidden_size      = 128\n",
    "# fc_hidden_size       = 64\n",
    "# num_layers           = 1\n",
    "# dropout_rate         = 0.3\n",
    "\n",
    "lr = best_params['lr']\n",
    "embed_dim = int(best_params['embed_dim'])\n",
    "rnn_hidden_size = int(best_params['rnn_hidden_size'])\n",
    "fc_hidden_size = int(best_params['fc_hidden_size'])\n",
    "rnn_hidden_size = int(best_params['num_layers'])\n",
    "fc_hidden_size = int(best_params['dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, \n",
    "                num_layers=num_layers, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Initialize lists to store training and validation metrics for each epoch\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "valid_accuracies = []\n",
    "valid_losses = []\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Final training loop\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train_epoch(train_dl, model, optimizer, loss_fn)\n",
    "    acc_valid, loss_valid = evaluate_epoch(val_dl, model, loss_fn)\n",
    "\n",
    "    # Store metrics\n",
    "    train_accuracies.append(acc_train)\n",
    "    train_losses.append(loss_train)\n",
    "    valid_accuracies.append(acc_valid)\n",
    "    valid_losses.append(loss_valid)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} - accuracy: {acc_train:.4f}, val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc12a0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9762fbe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting training and validation losses\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "# plt.savefig('images/lstm-action-accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "def generate_confusion_matrix(dataloader, model):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            preds = model(text_batch, lengths)[:, 0]\n",
    "            all_preds.extend((preds >= 0.5).cpu().numpy())\n",
    "            all_labels.extend(label_batch.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Human', 'Bot'], yticklabels=['Human', 'Bot'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Generate confusion matrix on the test dataset\n",
    "generate_confusion_matrix(test_dl, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "lstm-action-Copy5.ipynb",
   "output_path": "lstm-action-Copy5-pipeline.ipynb",
   "parameters": {},
   "start_time": "2024-12-14T20:12:02.107022",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
