{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to load the data\n",
    "def get_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "data = get_data('output_file copy.json')\n",
    "\n",
    "# Print original class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(data['user_class'].value_counts())\n",
    "\n",
    "# Separate data by user_class and balance\n",
    "bots = data[data['user_class'] == 'bot']\n",
    "humans = data[data['user_class'] == 'human']\n",
    "\n",
    "# Select the minimum class size and balance the dataset\n",
    "min_class_size = min(len(bots), len(humans))\n",
    "bots_balanced = bots.sample(n=min_class_size, random_state=1)\n",
    "humans_balanced = humans.sample(n=min_class_size, random_state=1)\n",
    "\n",
    "# Combine the balanced classes and shuffle\n",
    "balanced_data = pd.concat([bots_balanced, humans_balanced])\n",
    "balanced_data = shuffle(balanced_data, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Print balanced class distribution\n",
    "print(\"Balanced class distribution:\")\n",
    "print(balanced_data['user_class'].value_counts())\n",
    "\n",
    "# Define dataset class\n",
    "class UserDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.loc[idx, 'action_blocstring']\n",
    "        label = 1 if self.data.loc[idx, 'user_class'] == 'bot' else 0\n",
    "        return {\n",
    "            'text': text, \n",
    "            'label': label \n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = UserDataset(balanced_data)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Tokenizer to split text into individual characters\n",
    "def tokenizer(text):\n",
    "    return list(text)\n",
    "\n",
    "# Token counts\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "# Tokenize the training data\n",
    "for entry in train_dataset:\n",
    "    line = entry['text']\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Sort tokens by frequency\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# Create vocabulary dictionary with special tokens\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for idx, (token, count) in enumerate(ordered_dict.items(), start=2):\n",
    "    vocab[token] = idx\n",
    "\n",
    "# Print vocab size\n",
    "print('Vocab-size:', len(vocab))\n",
    "\n",
    "# Function to encode tokens\n",
    "def encode(tokens):\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "# Text pipeline for encoding tokens\n",
    "text_pipeline = lambda x: [vocab.get(token, vocab[\"<unk>\"]) for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: float(x)\n",
    "\n",
    "# Collate batch function\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for entry in batch:\n",
    "        _label = entry['label']\n",
    "        _text = entry['text']\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "\n",
    "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)\n",
    "\n",
    "# Attention-based LSTM Model\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size, num_layers=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.attn = nn.Linear(rnn_hidden_size, 1)  # Attention layer to calculate attention scores\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # Embedding layer\n",
    "        out = self.embedding(text)\n",
    "\n",
    "        # Pack the sequences to handle padding correctly\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        # RNN output\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "\n",
    "        # Unpack sequence (restore padded sequences)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        # Attention mechanism: Calculate attention scores\n",
    "        attention_weights = F.softmax(self.attn(out), dim=1)\n",
    "\n",
    "        # Weighted sum of the RNN outputs\n",
    "        weighted_sum = torch.sum(attention_weights * out, dim=1)\n",
    "\n",
    "        # Pass the weighted sum through the fully connected layers\n",
    "        out = self.fc1(weighted_sum)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 0.001\n",
    "embed_dim = 64\n",
    "rnn_hidden_size = 128\n",
    "fc_hidden_size = 64\n",
    "num_layers = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "model = AttentionLSTM(len(vocab), embed_dim, rnn_hidden_size, fc_hidden_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(text_batch, lengths)\n",
    "        loss = loss_fn(pred.squeeze(), label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "        total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_epoch(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            pred, _ = model(text_batch, lengths)\n",
    "            loss = loss_fn(pred.squeeze(), label_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == label_batch).float().sum().item()\n",
    "            total_loss += loss.item() * label_batch.size(0)\n",
    "    return total_acc / len(dataloader.dataset), total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Train the model with early stopping\n",
    "num_epochs = 40\n",
    "early_stop_patience = 10\n",
    "best_val_loss = 10\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "valid_accuracies = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train_epoch(train_loader, model, optimizer, loss_fn)\n",
    "    acc_valid, loss_valid = evaluate_epoch(val_loader, model, loss_fn)\n",
    "\n",
    "    train_accuracies.append(acc_train)\n",
    "    train_losses.append(loss_train)\n",
    "    valid_accuracies.append(acc_valid)\n",
    "    valid_losses.append(loss_valid)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} - train_accuracy: {acc_train:.4f}, val_accuracy: {acc_valid:.4f}, train_loss: {loss_train:.4f}, val_loss: {loss_valid:.4f}')\n",
    "\n",
    "    if loss_valid < best_val_loss:\n",
    "        best_val_loss = loss_valid\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}. Best validation loss was {best_val_loss:.4f} at epoch {best_epoch + 1}.\")\n",
    "        break\n",
    "\n",
    "# Plot the training and validation loss and accuracy\n",
    "epochs_completed = len(train_losses)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs_completed + 1), train_losses, label='Training Loss', color='red')\n",
    "plt.plot(range(1, epochs_completed + 1), valid_losses, label='Validation Loss', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs_completed + 1), train_accuracies, label='Training Accuracy', color='red')\n",
    "plt.plot(range(1, epochs_completed + 1), valid_accuracies, label='Validation Accuracy', color='blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloc-change",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
